# LinkedIn AI Intelligence Platform - Implementation Plan

**Status**: üöß In Progress
**Start Date**: October 16, 2025
**Goal**: Transform LinkedIn dashboard into an AI-queryable intelligence platform using Neon's native capabilities

---

## üìã Executive Summary

Enable AI tools (Claude, ChatGPT, Cursor, etc.) to directly query your LinkedIn post data via HTTPS, with semantic search capabilities and competitive intelligence insights - **without building a custom backend**.

### Key Benefits
- ‚úÖ **No backend code** - Neon Data API handles everything
- ‚úÖ **No context limits** - AI tools query only what they need
- ‚úÖ **Semantic search** - Find content by meaning, not just keywords
- ‚úÖ **Cost-effective** - ~$1/month total
- ‚úÖ **Low maintenance** - Native Neon features

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Neon PostgreSQL Database                     ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   posts     ‚îÇ  ‚îÇ   profiles   ‚îÇ  ‚îÇ  Intelligence Views  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (300+ rows)‚îÇ  ‚îÇ   (2 rows)   ‚îÇ  ‚îÇ  ‚Ä¢ top_performers    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ  ‚Ä¢ trending_hashtags ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ + embedding ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ  ‚Ä¢ competitive_intel ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   VECTOR    ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  Extensions: pgvector + pg_search                                ‚îÇ
‚îÇ  Features: Data API (PostgREST) + JWT Auth                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
                   HTTPS API (No Backend!)
                            ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì                   ‚Üì                   ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Claude  ‚îÇ        ‚îÇ ChatGPT ‚îÇ        ‚îÇ Cursor  ‚îÇ
   ‚îÇ   AI    ‚îÇ        ‚îÇ   API   ‚îÇ        ‚îÇ   IDE   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Current State Assessment

**Database**: Neon PostgreSQL (ep-wild-flower-adh2ui1j-pooler.c-2.us-east-1.aws.neon.tech)

### Posts Table
- **Total Posts**: 300
- **Profiles**:
  - Profile 1 (You): 200 posts
  - Profile 4 (Nicolas Boucher): 100 posts
- **Date Range**: 2016-09-08 to 2025-10-16
- **Fields**: id, post_url, content, author_name, likes, comments, shares, engagement_total, hashtags, published_at, profile_id

### Missing Components
- ‚ùå Embedding column (VECTOR type)
- ‚ùå pgvector extension
- ‚ùå Neon Data API enabled
- ‚ùå Intelligence views
- ‚ùå RLS policies

---

## üéØ Implementation Phases

### Phase 1: Enable Neon Data API ‚è±Ô∏è 30 minutes

#### Objective
Enable HTTPS query access to your database without custom backend code.

#### Steps

**1.1 Navigate to Neon Console**
- URL: https://console.neon.tech
- Select project: neondb
- Go to Settings ‚Üí Data API

**1.2 Enable Data API**
```
‚úì Enable Data API (toggle switch)
‚úì Choose authentication: Neon Auth (JWT)
‚úì Copy Data API endpoint URL
```

**Expected Output:**
```
Data API Endpoint: https://ep-wild-flower-adh2ui1j.data.neon.tech
```

**1.3 Generate JWT Token**
```bash
# In Neon console, generate JWT for authentication
# Token format: eyJhbGci... (long string)
```

**1.4 Test Basic Query**
```bash
# Test listing posts
curl "https://ep-wild-flower-adh2ui1j.data.neon.tech/posts?limit=5" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN"
```

**Expected Response:**
```json
[
  {
    "id": "7383869848457756672",
    "content_preview": "The CFO's real job isn't to predict...",
    "likes": 40,
    "comments": 14,
    "engagement_total": 54
  },
  ...
]
```

#### Deliverables
- ‚úÖ Data API endpoint URL
- ‚úÖ JWT authentication token
- ‚úÖ Test query successful
- ‚úÖ Document endpoint in `.env.local`

#### Cost
**$0** - Included with Neon

---

### Phase 2: Add pgvector + Generate Embeddings ‚è±Ô∏è 2 hours

#### Objective
Enable semantic search by adding vector embeddings to all posts.

#### Steps

**2.1 Enable pgvector Extension**
```sql
-- Run in Neon SQL Editor
CREATE EXTENSION IF NOT EXISTS vector;

-- Verify installation
SELECT * FROM pg_available_extensions WHERE name = 'vector';
```

**2.2 Add Embedding Column**
```sql
-- Add vector column (1536 dimensions for OpenAI text-embedding-3-small)
ALTER TABLE posts ADD COLUMN embedding VECTOR(1536);

-- Verify column added
\d posts
```

**2.3 Create Indexes**
```sql
-- HNSW index for fast similarity search (cosine distance)
CREATE INDEX posts_embedding_hnsw_idx ON posts
USING hnsw (embedding vector_cosine_ops);

-- Composite index for filtered searches
CREATE INDEX posts_profile_engagement_idx ON posts
(profile_id, engagement_total DESC, published_at DESC);

-- Index for hashtag queries
CREATE INDEX posts_hashtags_gin_idx ON posts USING gin (hashtags);
```

**2.4 Generate Embeddings Script**

Create `scripts/generate-embeddings.js`:
```javascript
import OpenAI from 'openai';
import { Pool } from 'pg';
import dotenv from 'dotenv';

dotenv.config({ path: '.env.local' });

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const pool = new Pool({ connectionString: process.env.DATABASE_URL });

async function generateEmbeddings() {
  console.log('üöÄ Starting embedding generation...\n');

  // Get posts without embeddings
  const { rows: posts } = await pool.query(`
    SELECT id, content, content_preview
    FROM posts
    WHERE embedding IS NULL
    ORDER BY published_at DESC
  `);

  console.log(`Found ${posts.length} posts needing embeddings\n`);

  let processed = 0;
  let failed = 0;

  for (const post of posts) {
    try {
      // Generate embedding
      const response = await openai.embeddings.create({
        model: "text-embedding-3-small",
        input: post.content || post.content_preview || '',
        encoding_format: "float"
      });

      const embedding = response.data[0].embedding;

      // Store in database
      await pool.query(
        'UPDATE posts SET embedding = $1 WHERE id = $2',
        [JSON.stringify(embedding), post.id]
      );

      processed++;
      console.log(`‚úì [${processed}/${posts.length}] Post ${post.id.slice(0, 10)}...`);

      // Rate limiting: OpenAI allows ~3000 requests/min
      if (processed % 50 === 0) {
        console.log('  ‚è≥ Pausing for rate limit...\n');
        await new Promise(resolve => setTimeout(resolve, 1000));
      }

    } catch (error) {
      failed++;
      console.error(`‚úó Failed for post ${post.id}: ${error.message}`);
    }
  }

  console.log(`\n‚ú® Complete! Processed: ${processed}, Failed: ${failed}`);
  console.log(`üí∞ Estimated cost: $${((processed * 0.00002) * 1536 / 1000).toFixed(4)}`);

  await pool.end();
}

generateEmbeddings();
```

**2.5 Run Embedding Generation**
```bash
cd /Users/saulmateos/Documents/GitHub/linkedin-dashboard

# Add OpenAI API key to .env.local if not present
echo "OPENAI_API_KEY=your_key_here" >> .env.local

# Install OpenAI package if needed
npm install openai

# Run script
node scripts/generate-embeddings.js
```

**Expected Output:**
```
üöÄ Starting embedding generation...

Found 300 posts needing embeddings

‚úì [1/300] Post 7383869848...
‚úì [2/300] Post 7382062279...
...
‚úì [300/300] Post 1234567890...

‚ú® Complete! Processed: 300, Failed: 0
üí∞ Estimated cost: $0.0184
```

**2.6 Verify Embeddings**
```sql
-- Check how many posts have embeddings
SELECT
  COUNT(*) as total_posts,
  COUNT(embedding) as posts_with_embeddings,
  COUNT(*) - COUNT(embedding) as posts_missing_embeddings
FROM posts;

-- Test semantic search
SELECT
  id,
  content_preview,
  engagement_total,
  1 - (embedding <=> '[0.1, 0.2, ...]'::vector) as similarity
FROM posts
WHERE embedding IS NOT NULL
ORDER BY similarity DESC
LIMIT 5;
```

#### Deliverables
- ‚úÖ pgvector extension enabled
- ‚úÖ 300 posts with embeddings
- ‚úÖ Indexes created
- ‚úÖ Semantic search working

#### Cost
- **OpenAI API**: ~$0.20 one-time
- **Storage**: ~1MB (negligible)

---

### Phase 3: Create Intelligence Views ‚è±Ô∏è 1 hour

#### Objective
Pre-build common analytical queries as SQL views for AI tools to access.

#### SQL Views Script

Create `scripts/create-intelligence-views.sql`:

```sql
-- ============================================================================
-- INTELLIGENCE VIEWS FOR AI TOOLS
-- ============================================================================

-- View 1: Top Performers by Profile
CREATE OR REPLACE VIEW top_performers AS
SELECT
  p.id as profile_id,
  p.display_name as profile_name,
  posts.id as post_id,
  posts.content_preview,
  posts.published_at,
  posts.likes,
  posts.comments,
  posts.shares,
  posts.engagement_total,
  posts.post_url
FROM posts
LEFT JOIN profiles p ON posts.profile_id = p.id
ORDER BY posts.engagement_total DESC;

COMMENT ON VIEW top_performers IS
'Top performing posts across all profiles, ordered by engagement';

-- View 2: Trending Hashtags (Last 30 Days)
CREATE MATERIALIZED VIEW trending_hashtags AS
SELECT
  hashtag,
  COUNT(*) as post_count,
  ROUND(AVG(engagement_total), 1) as avg_engagement,
  SUM(engagement_total) as total_engagement,
  ARRAY_AGG(DISTINCT profile_id) as profiles_using,
  MAX(published_at) as last_used
FROM posts, unnest(hashtags) as hashtag
WHERE published_at >= NOW() - INTERVAL '30 days'
GROUP BY hashtag
HAVING COUNT(*) >= 2
ORDER BY avg_engagement DESC;

COMMENT ON MATERIALIZED VIEW trending_hashtags IS
'Hashtag performance over last 30 days. Refresh: REFRESH MATERIALIZED VIEW trending_hashtags;';

-- View 3: Competitive Intelligence
CREATE OR REPLACE VIEW competitive_intel AS
SELECT
  p.id as profile_id,
  p.display_name as profile_name,
  p.profile_type,
  COUNT(posts.id) as total_posts,
  ROUND(AVG(posts.engagement_total), 1) as avg_engagement,
  MAX(posts.engagement_total) as best_post_engagement,
  ROUND(AVG(posts.likes), 1) as avg_likes,
  ROUND(AVG(posts.comments), 1) as avg_comments,
  COUNT(posts.id) FILTER (WHERE posts.published_at >= NOW() - INTERVAL '30 days') as posts_last_30d,
  ARRAY_AGG(DISTINCT hashtag ORDER BY hashtag) FILTER (WHERE hashtag IS NOT NULL) as top_hashtags
FROM profiles p
LEFT JOIN posts ON p.id = posts.profile_id
LEFT JOIN LATERAL unnest(posts.hashtags) as hashtag ON true
GROUP BY p.id, p.display_name, p.profile_type
ORDER BY avg_engagement DESC;

COMMENT ON VIEW competitive_intel IS
'Profile-level analytics for competitive comparison';

-- View 4: Content Performance by Topic
CREATE OR REPLACE VIEW topic_performance AS
SELECT
  hashtag as topic,
  COUNT(*) as post_count,
  ROUND(AVG(engagement_total), 1) as avg_engagement,
  MIN(engagement_total) as min_engagement,
  MAX(engagement_total) as max_engagement,
  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY engagement_total) as median_engagement
FROM posts, unnest(hashtags) as hashtag
GROUP BY hashtag
HAVING COUNT(*) >= 3
ORDER BY avg_engagement DESC;

COMMENT ON VIEW topic_performance IS
'Performance metrics by topic (hashtag)';

-- View 5: Posting Patterns
CREATE OR REPLACE VIEW posting_patterns AS
SELECT
  EXTRACT(DOW FROM published_at) as day_of_week,
  TO_CHAR(published_at, 'Day') as day_name,
  EXTRACT(HOUR FROM published_at) as hour_of_day,
  COUNT(*) as post_count,
  ROUND(AVG(engagement_total), 1) as avg_engagement,
  ARRAY_AGG(profile_id) as profiles
FROM posts
GROUP BY day_of_week, day_name, hour_of_day
HAVING COUNT(*) >= 2
ORDER BY avg_engagement DESC;

COMMENT ON VIEW posting_patterns IS
'Best days and times to post for maximum engagement';

-- View 6: Recent Activity (Last 7 Days)
CREATE OR REPLACE VIEW recent_activity AS
SELECT
  DATE(published_at) as date,
  COUNT(*) as posts_published,
  SUM(likes) as total_likes,
  SUM(comments) as total_comments,
  SUM(shares) as total_shares,
  ROUND(AVG(engagement_total), 1) as avg_engagement
FROM posts
WHERE published_at >= NOW() - INTERVAL '7 days'
GROUP BY DATE(published_at)
ORDER BY date DESC;

COMMENT ON VIEW recent_activity IS
'Daily activity summary for the last 7 days';

-- Grant access (adjust as needed for your RLS setup)
GRANT SELECT ON top_performers TO PUBLIC;
GRANT SELECT ON trending_hashtags TO PUBLIC;
GRANT SELECT ON competitive_intel TO PUBLIC;
GRANT SELECT ON topic_performance TO PUBLIC;
GRANT SELECT ON posting_patterns TO PUBLIC;
GRANT SELECT ON recent_activity TO PUBLIC;

-- Create refresh function for materialized views
CREATE OR REPLACE FUNCTION refresh_intelligence_views()
RETURNS void AS $$
BEGIN
  REFRESH MATERIALIZED VIEW trending_hashtags;
  RAISE NOTICE 'Intelligence views refreshed successfully';
END;
$$ LANGUAGE plpgsql;

COMMENT ON FUNCTION refresh_intelligence_views IS
'Refresh all materialized views used for intelligence. Run daily.';
```

**Run the script:**
```bash
# Via Neon SQL Editor or CLI
psql $DATABASE_URL -f scripts/create-intelligence-views.sql
```

**Test views:**
```sql
-- View top performers
SELECT * FROM top_performers LIMIT 10;

-- View trending hashtags
SELECT * FROM trending_hashtags;

-- Compare profiles
SELECT * FROM competitive_intel;
```

#### Deliverables
- ‚úÖ 6 intelligence views created
- ‚úÖ Views tested and verified
- ‚úÖ Refresh function created

#### Cost
**$0** - Native PostgreSQL

---

### Phase 4: AI Tool Integration ‚è±Ô∏è 1 hour

#### Objective
Document all endpoints and create ready-to-use examples for AI tools.

#### Create AI Query Guide

Create `docs/AI-QUERY-GUIDE.md`:

```markdown
# AI Query Guide for LinkedIn Intelligence

This guide provides ready-to-use queries for AI tools (Claude, ChatGPT, etc.) to access your LinkedIn data via Neon Data API.

## Authentication

All requests require JWT authentication:

```bash
export NEON_JWT="your_jwt_token_here"
export NEON_API="https://ep-wild-flower-adh2ui1j.data.neon.tech"
```

## Base Queries

### 1. Get Top Performing Posts

```bash
curl "$NEON_API/top_performers?limit=10" \
  -H "Authorization: Bearer $NEON_JWT"
```

**AI Prompt Example:**
> "What are my top 10 performing LinkedIn posts? For each, tell me what made it successful."

### 2. Trending Hashtags

```bash
curl "$NEON_API/trending_hashtags?order=avg_engagement.desc" \
  -H "Authorization: Bearer $NEON_JWT"
```

**AI Prompt Example:**
> "What hashtags are trending in my LinkedIn content? Which ones should I use more?"

### 3. Competitive Analysis

```bash
curl "$NEON_API/competitive_intel" \
  -H "Authorization: Bearer $NEON_JWT"
```

**AI Prompt Example:**
> "Compare my LinkedIn performance vs my competitors. Where am I winning? Where am I losing?"

### 4. Semantic Search

```bash
# First, get embedding for your query
curl https://api.openai.com/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "input": "AI automation in finance",
    "model": "text-embedding-3-small"
  }'

# Then query posts by similarity
curl "$NEON_API/posts?select=content_preview,engagement_total&order=embedding.dist($EMBEDDING)&limit=10" \
  -H "Authorization: Bearer $NEON_JWT"
```

**AI Prompt Example:**
> "Find my posts about AI automation in finance, ranked by relevance."

### 5. Content Performance by Topic

```bash
curl "$NEON_API/topic_performance?order=avg_engagement.desc&limit=20" \
  -H "Authorization: Bearer $NEON_JWT"
```

**AI Prompt Example:**
> "Which topics get the most engagement in my LinkedIn posts? Suggest 5 topics I should write about next."

### 6. Best Time to Post

```bash
curl "$NEON_API/posting_patterns?order=avg_engagement.desc&limit=10" \
  -H "Authorization: Bearer $NEON_JWT"
```

**AI Prompt Example:**
> "When should I post on LinkedIn for maximum engagement? Give me the top 3 day/time combinations."

### 7. Recent Activity

```bash
curl "$NEON_API/recent_activity" \
  -H "Authorization: Bearer $NEON_JWT"
```

**AI Prompt Example:**
> "Summarize my LinkedIn activity over the last 7 days. Am I posting consistently?"

## Advanced Queries

### Filter by Profile

```bash
# Get only your posts
curl "$NEON_API/posts?profile_id=eq.1&order=engagement_total.desc&limit=20" \
  -H "Authorization: Bearer $NEON_JWT"

# Get only competitor posts
curl "$NEON_API/posts?profile_id=eq.4&order=engagement_total.desc&limit=20" \
  -H "Authorization: Bearer $NEON_JWT"
```

### Filter by Date Range

```bash
# Posts from last 30 days
curl "$NEON_API/posts?published_at=gte.$(date -u -d '30 days ago' +%Y-%m-%d)&order=published_at.desc" \
  -H "Authorization: Bearer $NEON_JWT"
```

### Filter by Engagement Level

```bash
# High-performing posts only (>50 engagement)
curl "$NEON_API/posts?engagement_total=gte.50&order=engagement_total.desc" \
  -H "Authorization: Bearer $NEON_JWT"
```

### Search by Content

```bash
# Posts containing "AI" or "artificial intelligence"
curl "$NEON_API/posts?content=ilike.*AI*&order=engagement_total.desc" \
  -H "Authorization: Bearer $NEON_JWT"
```

## Integration with AI Tools

### Claude (Desktop or API)

Create a Custom Instruction or use in conversation:

```
You have access to my LinkedIn data via this API:

Endpoint: https://ep-wild-flower-adh2ui1j.data.neon.tech
Auth: Bearer [JWT_TOKEN]

Available endpoints:
- /top_performers - My best posts
- /trending_hashtags - Popular hashtags
- /competitive_intel - Comparison with competitors
- /topic_performance - Content performance by topic
- /posting_patterns - Best times to post
- /posts - Raw post data (filterable)

When I ask questions about my LinkedIn performance, query this API and provide insights.
```

### ChatGPT (Custom GPT)

Create a new GPT with this configuration:

**Instructions:**
```
You are a LinkedIn content strategist with access to the user's post data.

API Base URL: https://ep-wild-flower-adh2ui1j.data.neon.tech
Authentication: Include "Authorization: Bearer [JWT]" header

When asked about LinkedIn performance:
1. Query the appropriate endpoint
2. Analyze the data
3. Provide actionable insights
4. Suggest content ideas

Be specific and data-driven in your recommendations.
```

**Actions (OpenAPI Schema):**
```yaml
openapi: 3.0.0
info:
  title: LinkedIn Intelligence API
  version: 1.0.0
servers:
  - url: https://ep-wild-flower-adh2ui1j.data.neon.tech
paths:
  /top_performers:
    get:
      operationId: getTopPosts
      summary: Get top performing posts
      parameters:
        - name: limit
          in: query
          schema:
            type: integer
  /trending_hashtags:
    get:
      operationId: getTrendingHashtags
      summary: Get trending hashtags
  /competitive_intel:
    get:
      operationId: getCompetitiveIntel
      summary: Get competitive analysis
```

### Cursor IDE

Add to workspace settings (`.cursor/config.json`):

```json
{
  "apis": [
    {
      "name": "LinkedIn Data",
      "baseURL": "https://ep-wild-flower-adh2ui1j.data.neon.tech",
      "headers": {
        "Authorization": "Bearer YOUR_JWT_TOKEN"
      },
      "endpoints": {
        "topPosts": "/top_performers?limit=10",
        "trendingHashtags": "/trending_hashtags",
        "competitiveIntel": "/competitive_intel"
      }
    }
  ]
}
```

## Example AI Conversations

### Example 1: Content Strategy
**You:** "What should I post about next week?"

**AI:** *[Queries /trending_hashtags and /topic_performance]*

> Based on your data:
> - #AI posts average 52 engagement (your best performing topic)
> - #CFO posts get 38 engagement on average
> - #Leadership is declining (24 avg, down from 31 last month)
>
> Recommendation: Write 2 posts about practical AI applications for CFOs next week. Your audience responds best to specific, actionable content.

### Example 2: Competitive Gap
**You:** "How is Nicolas Boucher beating me in engagement?"

**AI:** *[Queries /competitive_intel and compares]*

> Analysis:
> - Nicolas: 156 avg engagement vs Your: 35 avg
> - His top topics: Excel + AI tutorials (visual guides)
> - Your top topics: Thought leadership (text-heavy)
> - His posting frequency: 3x per week vs Your: 1.2x per week
>
> Key differences:
> 1. More visual/practical content
> 2. Higher posting frequency
> 3. How-to format over opinion pieces
>
> Action plan:
> 1. Add screenshots/diagrams to your posts
> 2. Post 2-3x per week (Tuesday & Thursday mornings)
> 3. Mix in practical "how-to" content with your thought leadership

### Example 3: Best Practices
**You:** "What patterns exist in my best posts?"

**AI:** *[Queries /top_performers and analyzes content]*

> Your top 10 posts share these characteristics:
> 1. Personal stories (8/10 mention "I learned" or "Last week")
> 2. Specific numbers (7/10 include data/metrics)
> 3. Action-oriented (9/10 end with a question or call-to-action)
> 4. Posted Tuesday 9-11am (6/10)
> 5. Include 2-3 hashtags (never more)
>
> Template for success:
> [Personal anecdote] ‚Üí [Specific data] ‚Üí [Actionable insight] ‚Üí [Question to audience]

## Troubleshooting

### Error: 401 Unauthorized
- Check JWT token hasn't expired
- Verify token is included in Authorization header
- Regenerate token in Neon console if needed

### Error: 404 Not Found
- Verify endpoint name is correct (case-sensitive)
- Check if view exists: `SELECT * FROM pg_views WHERE viewname = 'your_view'`

### Slow Queries
- Ensure indexes are in place
- For large result sets, add `limit` parameter
- Consider refreshing materialized views

### Empty Results
- Check date filters (might be excluding all data)
- Verify profile_id filter if used
- Query raw posts table to confirm data exists

## Cost Monitoring

Every query costs $0 (Data API is free).

Embedding generation costs:
- Initial 300 posts: ~$0.20 one-time
- New posts: ~$0.0001 per post
- Monthly estimate: ~$0.02 (assuming 10 new posts/month)

## Next Steps

1. Set up automated weekly intelligence report
2. Create custom GPT for your LinkedIn strategy
3. Build Slack bot for team access
4. Expand competitor tracking (add more profiles)
```

#### Deliverables
- ‚úÖ Comprehensive AI query guide
- ‚úÖ Sample prompts for Claude/ChatGPT
- ‚úÖ Integration examples
- ‚úÖ Troubleshooting guide

---

### Phase 5: Automation (Optional) ‚è±Ô∏è 2 hours

#### Objective
Automate embedding generation for new posts and schedule view refreshes.

#### Auto-Embed New Posts

Update the scrape API to auto-generate embeddings.

Create `scripts/auto-embed-on-scrape.js`:

```javascript
import OpenAI from 'openai';
import { Pool } from 'pg';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const pool = new Pool({ connectionString: process.env.DATABASE_URL });

export async function embedNewPosts() {
  const { rows: newPosts } = await pool.query(`
    SELECT id, content, content_preview
    FROM posts
    WHERE embedding IS NULL
    ORDER BY scraped_at DESC
    LIMIT 50
  `);

  if (newPosts.length === 0) return { embedded: 0 };

  console.log(`Embedding ${newPosts.length} new posts...`);

  for (const post of newPosts) {
    try {
      const response = await openai.embeddings.create({
        model: "text-embedding-3-small",
        input: post.content || post.content_preview || ''
      });

      const embedding = response.data[0].embedding;

      await pool.query(
        'UPDATE posts SET embedding = $1 WHERE id = $2',
        [JSON.stringify(embedding), post.id]
      );

      console.log(`  ‚úì Embedded post ${post.id.slice(0, 10)}...`);
    } catch (error) {
      console.error(`  ‚úó Failed: ${error.message}`);
    }
  }

  return { embedded: newPosts.length };
}
```

**Integrate into scrape workflow:**

Edit `/Users/saulmateos/Documents/GitHub/linkedin-dashboard/app/api/scrape/route.ts`:

```typescript
// Add at the end of successful scrape
import { embedNewPosts } from '@/scripts/auto-embed-on-scrape';

// After database update complete:
console.log('Generating embeddings for new posts...');
const embedResult = await embedNewPosts();
console.log(`Embedded ${embedResult.embedded} posts`);
```

#### Schedule View Refreshes

Create cron job or GitHub Action:

```yaml
# .github/workflows/refresh-views.yml
name: Refresh Intelligence Views

on:
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight
  workflow_dispatch:

jobs:
  refresh:
    runs-on: ubuntu-latest
    steps:
      - name: Refresh Views
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          psql $DATABASE_URL -c "SELECT refresh_intelligence_views();"
```

#### Deliverables
- ‚úÖ Auto-embedding on scrape
- ‚úÖ Scheduled view refresh
- ‚úÖ Cost monitoring

---

## üì¶ Deliverables Checklist

### Documentation
- ‚úÖ PLAN.md (this file)
- ‚úÖ AI-QUERY-GUIDE.md
- ‚úÖ Updated CLAUDE.md (both projects)
- ‚úÖ README.md updates

### Scripts
- ‚úÖ `scripts/generate-embeddings.js`
- ‚úÖ `scripts/create-intelligence-views.sql`
- ‚úÖ `scripts/auto-embed-on-scrape.js`
- ‚úÖ `scripts/test-data-api.sh`

### Database
- ‚úÖ pgvector extension enabled
- ‚úÖ Embedding column added
- ‚úÖ Indexes created
- ‚úÖ 6 intelligence views
- ‚úÖ 300 posts with embeddings

### API
- ‚úÖ Neon Data API enabled
- ‚úÖ JWT authentication configured
- ‚úÖ All endpoints tested

---

## üí∞ Total Cost Breakdown

| Item | One-Time | Monthly | Notes |
|------|----------|---------|-------|
| Neon Database (Free Tier) | $0 | $0 | 0.5GB storage, 100 compute hours |
| Neon Data API | $0 | $0 | Included with free tier |
| pgvector Storage | $0 | $0 | ~1MB for 300 posts |
| OpenAI Embeddings (Initial) | $0.20 | - | 300 posts √ó $0.00067/1K tokens |
| OpenAI Embeddings (Ongoing) | - | $0.02 | ~10 new posts/month |
| **Total** | **$0.20** | **$0.02** | **Essentially free!** |

---

## üöÄ Success Metrics

### Technical
- ‚úÖ Query response time < 500ms
- ‚úÖ Embedding generation success rate > 99%
- ‚úÖ Data API uptime > 99.9%
- ‚úÖ Zero backend maintenance hours

### Business
- ‚úÖ AI tools can answer LinkedIn questions without context limits
- ‚úÖ Semantic search returns relevant results
- ‚úÖ Competitive intelligence queries work
- ‚úÖ Content strategy improves based on data

---

## üîÆ Future Enhancements

### Short Term (1-2 weeks)
1. Build Custom GPT for LinkedIn strategy
2. Add more competitor profiles (track 5-10 competitors)
3. Create Slack bot for team queries
4. Set up weekly intelligence email digest

### Medium Term (1-2 months)
1. Add sentiment analysis to posts
2. Track engagement velocity (early vs late engagement)
3. Build content recommendation engine
4. Create A/B testing framework for post formats

### Long Term (3+ months)
1. Multi-platform support (Twitter, Medium)
2. Automated content generation based on insights
3. Predictive engagement modeling
4. Integration with scheduling tools (Buffer, Hootsuite)

---

## üõ†Ô∏è Troubleshooting

### Common Issues

**Issue: Embeddings taking too long**
- Solution: Run in batches of 50 with rate limiting
- Check OpenAI API rate limits

**Issue: Data API not returning data**
- Check JWT token expiration
- Verify RLS policies if configured
- Test with curl first

**Issue: Semantic search not relevant**
- Re-generate embeddings with better content
- Adjust similarity threshold
- Use hybrid search (keyword + semantic)

**Issue: High costs**
- Embeddings are cached, only new posts cost money
- Monitor with OpenAI usage dashboard
- Typical usage: $0.02/month

---

## üìö Additional Resources

- [Neon Data API Docs](https://neon.tech/docs/data-api)
- [pgvector GitHub](https://github.com/pgvector/pgvector)
- [PostgREST API Reference](https://postgrest.org/en/stable/api.html)
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)

---

## üìû Support

For issues or questions:
1. Check this PLAN.md first
2. Review AI-QUERY-GUIDE.md
3. Test with curl before blaming AI tool
4. Check Neon console for database errors

---

**Last Updated**: October 16, 2025
**Status**: Ready for Phase 1 implementation
